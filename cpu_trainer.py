# CPU Modunda Model EÄŸitimi
import json
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    TrainingArguments, 
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import Dataset
import os
from datetime import datetime

class CPUTrainer:
    """
    CPU modunda model eÄŸitimi iÃ§in sÄ±nÄ±f
    """
    
    def __init__(self):
        self.model_name = "microsoft/DialoGPT-small"  # CPU iÃ§in kÃ¼Ã§Ã¼k model
        self.output_dir = "./cpu-trained-model"
        self.tokenizer = None
        self.model = None
        
    def load_model_and_tokenizer(self):
        """
        Model ve tokenizer'Ä± yÃ¼kler
        """
        print("ğŸ“¥ Model ve tokenizer yÃ¼kleniyor...")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForCausalLM.from_pretrained(self.model_name)
            
            # Pad token ekle
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            print("âœ… Model ve tokenizer baÅŸarÄ±yla yÃ¼klendi!")
            return True
            
        except Exception as e:
            print(f"âŒ Model yÃ¼kleme hatasÄ±: {e}")
            return False
    
    def prepare_dataset(self):
        """
        EÄŸitim verisini hazÄ±rlar
        """
        print("ğŸ“š EÄŸitim verisi hazÄ±rlanÄ±yor...")
        
        try:
            # EÄŸitim verisini yÃ¼kle
            with open("eÄŸitim_verisi.json", 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # Veriyi dÃ¶nÃ¼ÅŸtÃ¼r
            texts = []
            for item in data:
                instruction = item.get("instruction", "")
                input_text = item.get("input", "")
                output = item.get("output", "")
                
                # Basit format
                if input_text:
                    text = f"Soru: {instruction} {input_text} Cevap: {output}"
                else:
                    text = f"Soru: {instruction} Cevap: {output}"
                
                texts.append(text)
            
            # Dataset oluÅŸtur
            dataset = Dataset.from_dict({"text": texts})
            
            # Tokenize et
            def tokenize_function(examples):
                return self.tokenizer(
                    examples["text"], 
                    truncation=True, 
                    padding=True, 
                    max_length=512
                )
            
            tokenized_dataset = dataset.map(tokenize_function, batched=True)
            
            print(f"âœ… {len(texts)} Ã¶rnek hazÄ±rlandÄ±!")
            return tokenized_dataset
            
        except Exception as e:
            print(f"âŒ Veri hazÄ±rlama hatasÄ±: {e}")
            return None
    
    def train_model(self):
        """
        Modeli eÄŸitir
        """
        print("ğŸ‹ï¸ Model eÄŸitimi baÅŸlÄ±yor...")
        
        # Model ve tokenizer yÃ¼kle
        if not self.load_model_and_tokenizer():
            return False
        
        # Veriyi hazÄ±rla
        train_dataset = self.prepare_dataset()
        if train_dataset is None:
            return False
        
        # EÄŸitim parametreleri
        training_args = TrainingArguments(
            output_dir=self.output_dir,
            overwrite_output_dir=True,
            num_train_epochs=3,
            per_device_train_batch_size=2,
            save_steps=500,
            save_total_limit=2,
            prediction_loss_only=True,
            logging_steps=100,
            logging_dir='./logs',
            no_cuda=True,  # CPU modunda Ã§alÄ±ÅŸ
            dataloader_num_workers=0,  # Windows iÃ§in
            report_to=None  # Wandb kapalÄ±
        )
        
        # Data collator
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,
        )
        
        # Trainer oluÅŸtur
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            data_collator=data_collator,
        )
        
        try:
            # EÄŸitimi baÅŸlat
            print("â³ EÄŸitim baÅŸlatÄ±lÄ±yor... (Bu iÅŸlem uzun sÃ¼rebilir)")
            trainer.train()
            
            # Modeli kaydet
            print("ğŸ’¾ Model kaydediliyor...")
            trainer.save_model()
            self.tokenizer.save_pretrained(self.output_dir)
            
            print("âœ… EÄŸitim baÅŸarÄ±yla tamamlandÄ±!")
            print(f"ğŸ“ Model kaydedildi: {self.output_dir}")
            
            return True
            
        except Exception as e:
            print(f"âŒ EÄŸitim hatasÄ±: {e}")
            return False
    
    def test_model(self):
        """
        EÄŸitilmiÅŸ modeli test eder
        """
        if not os.path.exists(self.output_dir):
            print("âŒ EÄŸitilmiÅŸ model bulunamadÄ±!")
            return
        
        try:
            print("ğŸ§ª Model test ediliyor...")
            
            # EÄŸitilmiÅŸ modeli yÃ¼kle
            tokenizer = AutoTokenizer.from_pretrained(self.output_dir)
            model = AutoModelForCausalLM.from_pretrained(self.output_dir)
            
            # Test sorularÄ±
            test_questions = [
                "Soru: Yapay zeka nedir? Cevap:",
                "Soru: Python programlama dilinin avantajlarÄ± nelerdir? Cevap:",
                "Soru: SaÄŸlÄ±klÄ± beslenmenin Ã¶nemi nedir? Cevap:"
            ]
            
            print("\nğŸ¯ **Test SonuÃ§larÄ±:**")
            print("-" * 50)
            
            for i, question in enumerate(test_questions, 1):
                inputs = tokenizer.encode(question, return_tensors='pt')
                
                with torch.no_grad():
                    outputs = model.generate(
                        inputs, 
                        max_length=inputs.shape[1] + 100,
                        num_return_sequences=1,
                        temperature=0.7,
                        do_sample=True,
                        pad_token_id=tokenizer.eos_token_id
                    )
                
                response = tokenizer.decode(outputs[0], skip_special_tokens=True)
                answer = response[len(question):].strip()
                
                print(f"\n{i}. {question}")
                print(f"   YanÄ±t: {answer[:200]}...")
            
        except Exception as e:
            print(f"âŒ Test hatasÄ±: {e}")

def main():
    """
    Ana eÄŸitim fonksiyonu
    """
    print("ğŸš€ CPU Modunda Model EÄŸitimi BaÅŸlatÄ±lÄ±yor...")
    
    trainer = CPUTrainer()
    
    # EÄŸitimi baÅŸlat
    success = trainer.train_model()
    
    if success:
        # Modeli test et
        trainer.test_model()
        
        print("\nâœ… TÃ¼m iÅŸlemler tamamlandÄ±!")
        print("ğŸ’¡ ArtÄ±k AI sohbet modunda eÄŸitilmiÅŸ modeli kullanabilirsiniz.")
    else:
        print("\nâŒ EÄŸitim baÅŸarÄ±sÄ±z oldu!")

if __name__ == "__main__":
    main()