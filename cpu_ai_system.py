# CPU Modunda Tam AI Sistemi
import json
import os
import torch
from datetime import datetime
from typing import Dict, List
from transformers import AutoTokenizer, AutoModelForCausalLM

class CPUAISystem:
    """
    CPU modunda Ã§alÄ±ÅŸan tam AI sistemi
    """
    
    def __init__(self):
        print("ğŸ¤– CPU AI Sistemi baÅŸlatÄ±lÄ±yor...")
        
        # Arama motorunu ayarla
        self.setup_search_engine()
        
        # Model yollarÄ±
        self.trained_model_path = "./cpu-trained-model"
        self.default_model = "microsoft/DialoGPT-small"
        
        # Sistem bileÅŸenleri
        self.knowledge_base = self.load_knowledge_base()
        self.learning_history = []
        self.model = None
        self.tokenizer = None
        
        # Modeli yÃ¼kle
        self.load_model()
    
    def setup_search_engine(self):
        """
        Arama motorunu ayarlar
        """
        try:
            from api_config import is_api_configured
            if is_api_configured():
                print("ğŸ” Google Search API kullanÄ±lÄ±yor")
                from search_engine import GoogleSearchEngine
                self.search_engine = GoogleSearchEngine()
                self.search_type = "google"
            else:
                print("ğŸ” Ãœcretsiz arama motoru kullanÄ±lÄ±yor")
                from free_search_engine import FreeSearchEngine
                self.search_engine = FreeSearchEngine()
                self.search_type = "free"
        except Exception as e:
            print(f"âš ï¸ Arama motoru ayarlanÄ±rken hata: {e}")
            from free_search_engine import FreeSearchEngine
            self.search_engine = FreeSearchEngine()
            self.search_type = "free"
    
    def load_model(self):
        """
        AI modelini yÃ¼kler
        """
        try:
            # Ã–nce eÄŸitilmiÅŸ modeli dene
            if os.path.exists(self.trained_model_path):
                print("ğŸ“¥ EÄŸitilmiÅŸ model yÃ¼kleniyor...")
                self.tokenizer = AutoTokenizer.from_pretrained(self.trained_model_path)
                self.model = AutoModelForCausalLM.from_pretrained(self.trained_model_path)
                print("âœ… EÄŸitilmiÅŸ model yÃ¼klendi!")
            else:
                print("ğŸ“¥ VarsayÄ±lan model yÃ¼kleniyor...")
                self.tokenizer = AutoTokenizer.from_pretrained(self.default_model)
                self.model = AutoModelForCausalLM.from_pretrained(self.default_model)
                
                # Pad token ekle
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                
                print("âœ… VarsayÄ±lan model yÃ¼klendi!")
                
        except Exception as e:
            print(f"âŒ Model yÃ¼kleme hatasÄ±: {e}")
            print("ğŸ’¡ Basit metin tabanlÄ± modda Ã§alÄ±ÅŸacak")
            self.model = None
            self.tokenizer = None
    
    def load_knowledge_base(self) -> Dict:
        """
        Bilgi tabanÄ±nÄ± yÃ¼kler
        """
        kb_file = "knowledge_base.json"
        if os.path.exists(kb_file):
            with open(kb_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {
            "learned_facts": {},
            "search_history": [],
            "improvement_log": [],
            "last_update": datetime.now().isoformat()
        }
    
    def save_knowledge_base(self):
        """
        Bilgi tabanÄ±nÄ± kaydeder
        """
        self.knowledge_base["last_update"] = datetime.now().isoformat()
        with open("knowledge_base.json", 'w', encoding='utf-8') as f:
            json.dump(self.knowledge_base, f, ensure_ascii=False, indent=2)
    
    def generate_ai_response(self, question: str) -> str:
        """
        AI modeli ile yanÄ±t Ã¼retir
        """
        if self.model is None or self.tokenizer is None:
            return self.generate_rule_based_response(question)
        
        try:
            # Prompt hazÄ±rla
            prompt = f"Soru: {question} Cevap:"
            
            # Tokenize et
            inputs = self.tokenizer.encode(prompt, return_tensors='pt')
            
            # YanÄ±t Ã¼ret
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_length=inputs.shape[1] + 150,
                    num_return_sequences=1,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    no_repeat_ngram_size=2
                )
            
            # Decode et
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            answer = response[len(prompt):].strip()
            
            # BoÅŸ yanÄ±t kontrolÃ¼
            if not answer or len(answer) < 10:
                return self.generate_rule_based_response(question)
            
            return answer
            
        except Exception as e:
            print(f"âš ï¸ AI model hatasÄ±: {e}")
            return self.generate_rule_based_response(question)
    
    def generate_rule_based_response(self, question: str) -> str:
        """
        Kural tabanlÄ± yanÄ±t Ã¼retir
        """
        question_lower = question.lower()
        
        # Selamlama kontrolÃ¼
        greetings = ["merhaba", "selam", "iyi gÃ¼nler", "nasÄ±lsÄ±n", "hey"]
        if any(greeting in question_lower for greeting in greetings):
            return "Merhaba! Ben bir AI asistanÄ±yÄ±m. Size nasÄ±l yardÄ±mcÄ± olabilirim?"
        
        # Bilgi tabanÄ±nda ara
        cached_response = self.search_knowledge_base(question)
        if cached_response:
            return f"{cached_response}\n\n[Kaynak: Ã–ÄŸrenilmiÅŸ Bilgi TabanÄ±]"
        
        # EÄŸitim verisinde ara
        training_response = self.search_training_data(question)
        if training_response:
            return training_response
        
        return "Bu konu hakkÄ±nda daha fazla bilgi edinmek iÃ§in gÃ¼ncel arama yapayÄ±m."
    
    def search_knowledge_base(self, query: str) -> str:
        """
        Bilgi tabanÄ±nda arama yapar
        """
        query_lower = query.lower()
        for fact_key, fact_value in self.knowledge_base["learned_facts"].items():
            if any(keyword in query_lower for keyword in fact_key.lower().split()):
                return fact_value["content"]
        return None
    
    def search_training_data(self, question: str) -> str:
        """
        EÄŸitim verisinde arama yapar
        """
        try:
            with open("eÄŸitim_verisi.json", 'r', encoding='utf-8') as f:
                training_data = json.load(f)
            
            question_lower = question.lower()
            best_match = None
            best_score = 0
            
            for item in training_data:
                instruction = item.get("instruction", "").lower()
                input_text = item.get("input", "").lower()
                
                question_words = set(question_lower.split())
                instruction_words = set(instruction.split())
                input_words = set(input_text.split())
                
                instruction_score = len(question_words.intersection(instruction_words))
                input_score = len(question_words.intersection(input_words))
                total_score = instruction_score + input_score
                
                if total_score > best_score:
                    best_score = total_score
                    best_match = item
            
            if best_match and best_score > 0:
                return best_match.get("output", "")
            
        except Exception as e:
            print(f"EÄŸitim verisi arama hatasÄ±: {e}")
        
        return None
    
    def enhance_with_search(self, question: str) -> str:
        """
        Arama ile yanÄ±tÄ± geliÅŸtirir
        """
        print(f"ğŸ” '{question}' iÃ§in gÃ¼ncel bilgi aranÄ±yor...")
        
        # AI yanÄ±tÄ± Ã¼ret
        ai_response = self.generate_ai_response(question)
        
        try:
            # Arama yap
            if self.search_type == "google":
                search_results = self.search_engine.search(question, num_results=3)
            else:
                search_results = self.search_engine.comprehensive_search(question, num_results=3)
            
            if search_results:
                search_info = "\n\nğŸ“Š **GÃ¼ncel Bilgiler:**\n"
                for i, result in enumerate(search_results, 1):
                    search_info += f"{i}. **{result.get('title', 'BaÅŸlÄ±k yok')}**\n"
                    search_info += f"   {result.get('snippet', 'AÃ§Ä±klama yok')}\n"
                    if result.get('link'):
                        search_info += f"   ğŸ”— {result['link']}\n"
                    search_info += "\n"
                
                # Bilgi tabanÄ±nÄ± gÃ¼ncelle
                self.learn_new_information(question, search_info, "search_enhancement")
                
                enhanced_response = ai_response + search_info
                enhanced_response += f"\nğŸ•’ **Son GÃ¼ncelleme:** {datetime.now().strftime('%Y-%m-%d %H:%M')}"
                
                return enhanced_response
            
        except Exception as e:
            print(f"Arama hatasÄ±: {e}")
        
        return ai_response
    
    def learn_new_information(self, topic: str, information: str, source: str):
        """
        Yeni bilgiyi Ã¶ÄŸrenir
        """
        learning_entry = {
            "topic": topic,
            "information": information,
            "source": source,
            "learned_at": datetime.now().isoformat()
        }
        
        topic_key = topic.lower().replace(" ", "_")
        self.knowledge_base["learned_facts"][topic_key] = {
            "content": information,
            "source": source,
            "learned_at": datetime.now().isoformat()
        }
        
        self.learning_history.append(learning_entry)
        self.save_knowledge_base()
        
        print(f"âœ… Yeni bilgi Ã¶ÄŸrenildi: {topic}")
    
    def self_improve(self):
        """
        Kendini geliÅŸtirme sÃ¼reci
        """
        print("ğŸš€ Kendini geliÅŸtirme sÃ¼reci baÅŸlatÄ±lÄ±yor...")
        
        popular_topics = [
            "yapay zeka son geliÅŸmeler",
            "teknoloji haberleri 2024",
            "Python programlama yenilikleri",
            "bilim son dakika",
            "blockchain teknolojisi"
        ]
        
        improved_count = 0
        for topic in popular_topics:
            try:
                current_info = self.search_engine.get_current_info(topic)
                if current_info and len(current_info) > 50:
                    self.learn_new_information(topic, current_info, "self_improvement")
                    print(f"ğŸ“š {topic} hakkÄ±nda bilgi gÃ¼ncellendi")
                    improved_count += 1
            except Exception as e:
                print(f"âŒ {topic} gÃ¼ncellenirken hata: {e}")
        
        # Ä°yileÅŸtirme logunu gÃ¼ncelle
        improvement_log = {
            "timestamp": datetime.now().isoformat(),
            "topics_updated": improved_count,
            "total_learned_facts": len(self.knowledge_base["learned_facts"])
        }
        
        self.knowledge_base["improvement_log"].append(improvement_log)
        self.save_knowledge_base()
        
        print(f"âœ¨ Kendini geliÅŸtirme sÃ¼reci tamamlandÄ±! {improved_count} konu gÃ¼ncellendi.")
    
    def get_stats(self) -> Dict:
        """
        Sistem istatistikleri
        """
        model_info = "EÄŸitilmiÅŸ Model" if os.path.exists(self.trained_model_path) else "VarsayÄ±lan Model"
        if self.model is None:
            model_info = "Metin TabanlÄ± Mod"
        
        return {
            "total_learned_facts": len(self.knowledge_base["learned_facts"]),
            "learning_sessions": len(self.learning_history),
            "last_update": self.knowledge_base.get("last_update", "HiÃ§ gÃ¼ncellenmedi"),
            "search_type": self.search_type,
            "model_type": model_info,
            "system_mode": "CPU Modu",
            "improvement_sessions": len(self.knowledge_base.get("improvement_log", []))
        }
    
    def interactive_chat(self):
        """
        EtkileÅŸimli sohbet modu
        """
        print("ğŸ¤– CPU AI Sistemi Aktif!")
        print("Komutlar: 'Ã§Ä±kÄ±ÅŸ' - Ã§Ä±k, 'istatistik' - stats, 'geliÅŸtir' - self improve")
        print("-" * 50)
        
        while True:
            user_input = input("\nğŸ‘¤ Siz: ").strip()
            
            if user_input.lower() in ['Ã§Ä±kÄ±ÅŸ', 'exit', 'quit']:
                print("ğŸ‘‹ GÃ¶rÃ¼ÅŸmek Ã¼zere!")
                break
            elif user_input.lower() in ['istatistik', 'stats']:
                stats = self.get_stats()
                print("\nğŸ“Š **Sistem Ä°statistikleri:**")
                for key, value in stats.items():
                    print(f"   {key}: {value}")
            elif user_input.lower() in ['geliÅŸtir', 'improve']:
                self.self_improve()
            elif user_input.lower().startswith('ara:'):
                search_query = user_input[4:].strip()
                response = self.enhance_with_search(search_query)
                print(f"\nğŸ¤– AI: {response}")
            else:
                # Normal soru-cevap
                if "gÃ¼ncel" in user_input.lower() or "son" in user_input.lower():
                    response = self.enhance_with_search(user_input)
                else:
                    response = self.generate_ai_response(user_input)
                
                print(f"\nğŸ¤– AI: {response}")

def main():
    """
    Ana program
    """
    print("ğŸš€ CPU AI Sistemi BaÅŸlatÄ±lÄ±yor...")
    
    try:
        ai_system = CPUAISystem()
        ai_system.interactive_chat()
    except Exception as e:
        print(f"âŒ Sistem hatasÄ±: {e}")

if __name__ == "__main__":
    main()